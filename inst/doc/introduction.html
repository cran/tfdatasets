<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>R interface to TensorFlow Dataset API</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">R interface to TensorFlow Dataset API</h1>



<div id="overview" class="section level2">
<h2>Overview</h2>
<p>The TensorFlow Dataset API provides various facilities for creating scalable input pipelines for TensorFlow models, including:</p>
<ul>
<li><p>Reading data from a variety of formats including CSV files and <a href="https://www.tensorflow.org/api_guides/python/python_io#tfrecords_format_details">TFRecords files</a> (the standard binary format for TensorFlow training data).</p></li>
<li><p>Transforming datasets in a variety of ways including mapping arbitrary functions against them.</p></li>
<li><p>Shuffling, batching, and repeating datasets over a number of epochs.</p></li>
<li><p>Streaming interface to data for reading arbitrarily large datasets.</p></li>
<li><p>Reading and transforming data are TensorFlow graph operations, so are executed in C++ and in parallel with model training.</p></li>
</ul>
<p>The R interface to TensorFlow datasets provides access to the Dataset API, including high-level convenience functions for easy integration with the <a href="https://tensorflow.rstudio.com/keras">keras</a> R package.</p>
</div>
<div id="installation" class="section level2">
<h2>Installation</h2>
<p>To use <strong>tfdatasets</strong> you need to install both the R package as well as <a href="https://rstudio.github.io/tensorflow/">TensorFlow</a> itself.</p>
<p>First, install the tfdatasets R package from GitHub as follows:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a>devtools<span class="op">::</span><span class="kw">install_github</span>(<span class="st">&quot;rstudio/tfdatasets&quot;</span>)</span></code></pre></div>
<p>Then, use the <code>install_tensorflow()</code> function to install TensorFlow:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a><span class="kw">library</span>(tfdtasets)</span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="kw">install_tensorflow</span>()</span></code></pre></div>
</div>
<div id="creating-a-dataset" class="section level2">
<h2>Creating a Dataset</h2>
<p>To create a dataset, use one of the <a href="https://tensorflow.rstudio.com/reference/tfdatasets/#section-creating-datasets">dataset creation</a> functions. Dataset can be created from delimted text files, <a href="https://www.tensorflow.org/api_guides/python/python_io#tfrecords_format_details">TFRecords files</a>, as well as from in-memory data.</p>
<div id="text-files" class="section level3">
<h3>Text Files</h3>
<p>For example, to create a dataset from a text file, first create a specification for how records will be decoded from the file, then call <code>text_line_dataset()</code> with the file to be read and the specification:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a><span class="kw">library</span>(tfdatasets)</span>
<span id="cb3-2"><a href="#cb3-2"></a></span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="co"># create specification for parsing records from an example file</span></span>
<span id="cb3-4"><a href="#cb3-4"></a>iris_spec &lt;-<span class="st"> </span><span class="kw">csv_record_spec</span>(<span class="st">&quot;iris.csv&quot;</span>)</span>
<span id="cb3-5"><a href="#cb3-5"></a></span>
<span id="cb3-6"><a href="#cb3-6"></a><span class="co"># read the datset</span></span>
<span id="cb3-7"><a href="#cb3-7"></a>dataset &lt;-<span class="st"> </span><span class="kw">text_line_dataset</span>(<span class="st">&quot;iris.csv&quot;</span>, <span class="dt">record_spec =</span> iris_spec) </span>
<span id="cb3-8"><a href="#cb3-8"></a></span>
<span id="cb3-9"><a href="#cb3-9"></a><span class="co"># take a glimpse at the dataset</span></span>
<span id="cb3-10"><a href="#cb3-10"></a><span class="kw">str</span>(dataset)</span></code></pre></div>
<pre><code>&lt;MapDataset shapes: {Sepal.Length: (), Sepal.Width: (), Petal.Length: (), Petal.Width: (),
Species: ()}, types: {Sepal.Length: tf.float32, Sepal.Width: tf.float32, Petal.Length:
tf.float32, Petal.Width: tf.float32, Species: tf.int32}&gt;</code></pre>
<p>In the example above, the <code>csv_record_spec()</code> function is passed an example file which is used to automatically detect column names and types (done by reading up to the first 1,000 lines of the file). You can also provide explicit column names and/or data types using the <code>names</code> and <code>types</code> parameters (note that in this case we don’t pass an example file):</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a><span class="co"># provide colum names and types explicitly</span></span>
<span id="cb5-2"><a href="#cb5-2"></a>iris_spec &lt;-<span class="st"> </span><span class="kw">csv_record_spec</span>(</span>
<span id="cb5-3"><a href="#cb5-3"></a>  <span class="dt">names =</span> <span class="kw">c</span>(<span class="st">&quot;SepalLength&quot;</span>, <span class="st">&quot;SepalWidth&quot;</span>, <span class="st">&quot;PetalLength&quot;</span>, <span class="st">&quot;PetalWidth&quot;</span>, <span class="st">&quot;Species&quot;</span>),</span>
<span id="cb5-4"><a href="#cb5-4"></a>  <span class="dt">types =</span> <span class="kw">c</span>(<span class="st">&quot;double&quot;</span>, <span class="st">&quot;double&quot;</span>, <span class="st">&quot;double&quot;</span>, <span class="st">&quot;double&quot;</span>, <span class="st">&quot;integer&quot;</span>), </span>
<span id="cb5-5"><a href="#cb5-5"></a>  <span class="dt">skip =</span> <span class="dv">1</span></span>
<span id="cb5-6"><a href="#cb5-6"></a>)</span>
<span id="cb5-7"><a href="#cb5-7"></a></span>
<span id="cb5-8"><a href="#cb5-8"></a><span class="co"># read the datset</span></span>
<span id="cb5-9"><a href="#cb5-9"></a>dataset &lt;-<span class="st"> </span><span class="kw">text_line_dataset</span>(<span class="st">&quot;iris.csv&quot;</span>, <span class="dt">record_spec =</span> iris_spec)</span></code></pre></div>
<p>Note that we’ve also specified <code>skip = 1</code> to indicate that the first row of the CSV that contains column names should be skipped.</p>
<p>Supported column types are integer, double, and character. You can also provide <code>types</code> in a more compact form using single-letter abbreviations (e.g. <code>types = &quot;dddi&quot;</code>). For example:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a>mtcars_spec &lt;-<span class="st"> </span><span class="kw">csv_record_spec</span>(<span class="st">&quot;mtcars.csv&quot;</span>, <span class="dt">types =</span> <span class="st">&quot;dididddiiii&quot;</span>)</span></code></pre></div>
<div id="parallel-decoding" class="section level4">
<h4>Parallel Decoding</h4>
<p>Decoding lines of text into a record can be computationally expensive. You can parallelize these computations using the <code>parallel_records</code> parameter. For example:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1"></a>dataset &lt;-<span class="st"> </span><span class="kw">text_line_dataset</span>(<span class="st">&quot;iris.csv&quot;</span>, <span class="dt">record_spec =</span> iris_spec, <span class="dt">parallel_records =</span> <span class="dv">4</span>)</span></code></pre></div>
<p>You can also parallelize the reading of data from storage by requesting that a buffer of records be prefected. You do this with the <code>dataset_prefetch()</code> function. For example:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a>dataset &lt;-<span class="st"> </span><span class="kw">text_line_dataset</span>(<span class="st">&quot;iris.csv&quot;</span>, <span class="dt">record_spec =</span> iris_spec, <span class="dt">parallel_records =</span> <span class="dv">4</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="st">  </span><span class="kw">dataset_batch</span>(<span class="dv">128</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="st">  </span><span class="kw">dataset_prefetch</span>(<span class="dv">1</span>)</span></code></pre></div>
<p>This code will result in the prefetching of a single batch of data on a background thread (i.e. in parallel with training operations).</p>
<p>If you have multiple input files, you can also parallelize reading of these files both across multiple machines (sharding) and/or on multiple threads per-machine (parallel reads with interleaving). See the section on <a href="#reading-multiple-files">Reading Multiple Files</a> below for additional details.</p>
</div>
</div>
<div id="tfrecords-files" class="section level3">
<h3>TFRecords Files</h3>
<p>You can read datasets from <a href="https://www.tensorflow.org/api_guides/python/python_io#tfrecords_format_details">TFRecords files</a> using the <code>tfrecord_dataset()</code> function.</p>
<p>In many cases you’ll want to map the records in the dataset into a set of named columns. You can do this using the <code>dataset_map()</code> function along with the <code>tf$parse_single_example()</code> function. for example:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a><span class="co"># Creates a dataset that reads all of the examples from two files, and extracts</span></span>
<span id="cb9-2"><a href="#cb9-2"></a><span class="co"># the image and label features.</span></span>
<span id="cb9-3"><a href="#cb9-3"></a>filenames &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;/var/data/file1.tfrecord&quot;</span>, <span class="st">&quot;/var/data/file2.tfrecord&quot;</span>)</span>
<span id="cb9-4"><a href="#cb9-4"></a>dataset &lt;-<span class="st"> </span><span class="kw">tfrecord_dataset</span>(filenames) <span class="op">%&gt;%</span></span>
<span id="cb9-5"><a href="#cb9-5"></a><span class="st">  </span><span class="kw">dataset_map</span>(<span class="cf">function</span>(example_proto) {</span>
<span id="cb9-6"><a href="#cb9-6"></a>    features &lt;-<span class="st"> </span><span class="kw">list</span>(</span>
<span id="cb9-7"><a href="#cb9-7"></a>      <span class="dt">image =</span> tf<span class="op">$</span><span class="kw">FixedLenFeature</span>(<span class="kw">shape</span>(), tf<span class="op">$</span>string),</span>
<span id="cb9-8"><a href="#cb9-8"></a>      <span class="dt">label =</span> tf<span class="op">$</span><span class="kw">FixedLenFeature</span>(<span class="kw">shape</span>(), tf<span class="op">$</span>int32)</span>
<span id="cb9-9"><a href="#cb9-9"></a>    )</span>
<span id="cb9-10"><a href="#cb9-10"></a>    tf<span class="op">$</span><span class="kw">parse_single_example</span>(example_proto, features)</span>
<span id="cb9-11"><a href="#cb9-11"></a>  })</span></code></pre></div>
<p>You can parallelize reading of TFRecord files using the <code>num_parallel_reads</code> option, for example:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a>filenames &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;/var/data/file1.tfrecord&quot;</span>, <span class="st">&quot;/var/data/file2.tfrecord&quot;</span>)</span>
<span id="cb10-2"><a href="#cb10-2"></a>dataset &lt;-<span class="st"> </span><span class="kw">tfrecord_dataset</span>(filenames, <span class="dt">num_parallel_reads =</span> <span class="dv">4</span>)</span></code></pre></div>
</div>
</div>
<div id="sqlite-databases" class="section level2">
<h2>SQLite Databases</h2>
<p>You can read datasets from SQLite databases using the <code>sqlite_dataset()</code> function. To use <code>sqlite_dataset()</code> you provide the filename of the database, a SQL query to execute, and <code>sql_record_spec()</code> that describes the names and TensorFlow types of columns within the query. For example:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1"></a><span class="kw">library</span>(tfdatasets)</span>
<span id="cb11-2"><a href="#cb11-2"></a></span>
<span id="cb11-3"><a href="#cb11-3"></a>record_spec &lt;-<span class="st"> </span><span class="kw">sql_record_spec</span>(</span>
<span id="cb11-4"><a href="#cb11-4"></a>  <span class="dt">names =</span> <span class="kw">c</span>(<span class="st">&quot;disp&quot;</span>, <span class="st">&quot;drat&quot;</span>, <span class="st">&quot;vs&quot;</span>, <span class="st">&quot;gear&quot;</span>, <span class="st">&quot;mpg&quot;</span>, <span class="st">&quot;qsec&quot;</span>, <span class="st">&quot;hp&quot;</span>, <span class="st">&quot;am&quot;</span>, <span class="st">&quot;wt&quot;</span>,  <span class="st">&quot;carb&quot;</span>, <span class="st">&quot;cyl&quot;</span>),</span>
<span id="cb11-5"><a href="#cb11-5"></a>  <span class="dt">types =</span> <span class="kw">c</span>(tf<span class="op">$</span>float64, tf<span class="op">$</span>int32, tf<span class="op">$</span>float64, tf<span class="op">$</span>int32, tf<span class="op">$</span>float64, tf<span class="op">$</span>float64,</span>
<span id="cb11-6"><a href="#cb11-6"></a>            tf<span class="op">$</span>float64, tf<span class="op">$</span>int32, tf<span class="op">$</span>int32, tf<span class="op">$</span>int32, tf<span class="op">$</span>int32)</span>
<span id="cb11-7"><a href="#cb11-7"></a>)</span>
<span id="cb11-8"><a href="#cb11-8"></a></span>
<span id="cb11-9"><a href="#cb11-9"></a>dataset &lt;-<span class="st"> </span><span class="kw">sqlite_dataset</span>(</span>
<span id="cb11-10"><a href="#cb11-10"></a>  <span class="st">&quot;data/mtcars.sqlite3&quot;</span>,</span>
<span id="cb11-11"><a href="#cb11-11"></a>  <span class="st">&quot;select * from mtcars&quot;</span>,</span>
<span id="cb11-12"><a href="#cb11-12"></a>  record_spec</span>
<span id="cb11-13"><a href="#cb11-13"></a>)</span>
<span id="cb11-14"><a href="#cb11-14"></a></span>
<span id="cb11-15"><a href="#cb11-15"></a>dataset</span></code></pre></div>
<pre><code>&lt;MapDataset shapes: {disp: (), drat: (), vs: (), gear: (), mpg: (), qsec: (), hp: (), am: (),
wt: (), carb: (), cyl: ()}, types: {disp: tf.float64, drat: tf.int32, vs: tf.float64, gear:
tf.int32, mpg: tf.float64, qsec: tf.float64, hp: tf.float64, am: tf.int32, wt: tf.int32, carb:
tf.int32, cyl: tf.int32}&gt;</code></pre>
<p>Note that for floating point data you must use <code>tf$float64</code> (reading <code>tf$float32</code> is not supported for SQLite databases).</p>
</div>
<div id="transformations" class="section level2">
<h2>Transformations</h2>
<div id="mapping" class="section level3">
<h3>Mapping</h3>
<p>You can map arbitrary transformation functions onto dataset records using the <code>dataset_map()</code> function. For example, to transform the “Species” column into a one-hot encoded vector you would do this:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a>dataset &lt;-<span class="st"> </span>dataset <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb13-2"><a href="#cb13-2"></a><span class="st">  </span><span class="kw">dataset_map</span>(<span class="cf">function</span>(record) {</span>
<span id="cb13-3"><a href="#cb13-3"></a>    record<span class="op">$</span>Species &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">one_hot</span>(record<span class="op">$</span>Species, 3L)</span>
<span id="cb13-4"><a href="#cb13-4"></a>    record</span>
<span id="cb13-5"><a href="#cb13-5"></a>  })</span></code></pre></div>
<p>Note that while <code>dataset_map()</code> is defined using an R function, there are some special constraints on this function which allow it to execute <em>not within R</em> but rather within the TensorFlow graph.</p>
<p>For a dataset created with the <code>csv_dataset()</code> function, the passed record will be named list of tensors (one for each column of the dataset). The return value should be another set of tensors which were created from TensorFlow functions (e.g. <code>tf$one_hot</code> as illustrated above). This function will be converted to a TensorFlow graph operation that performs the transformation within native code.</p>
<div id="parallel-mapping" class="section level4">
<h4>Parallel Mapping</h4>
<p>If these transformations are computationally expensive they can be executed on multiple threads using the <code>num_parallel_calls</code> parameter. For example:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1"></a>dataset &lt;-<span class="st"> </span>dataset <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb14-2"><a href="#cb14-2"></a><span class="st">  </span><span class="kw">dataset_map</span>(<span class="dt">num_parallel_calls =</span> <span class="dv">4</span>, <span class="cf">function</span>(record) {</span>
<span id="cb14-3"><a href="#cb14-3"></a>    record<span class="op">$</span>Species &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">one_hot</span>(record<span class="op">$</span>Species, 3L)</span>
<span id="cb14-4"><a href="#cb14-4"></a>    record</span>
<span id="cb14-5"><a href="#cb14-5"></a>  })</span></code></pre></div>
<p>You can control the maximum number of processed elements that will be buffered when processing in parallel using the <code>dataset_prefetch()</code> transformation. For example:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1"></a>dataset &lt;-<span class="st"> </span>dataset <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb15-2"><a href="#cb15-2"></a><span class="st">  </span><span class="kw">dataset_map</span>(<span class="dt">num_parallel_calls =</span> <span class="dv">4</span>, <span class="cf">function</span>(record) {</span>
<span id="cb15-3"><a href="#cb15-3"></a>    record<span class="op">$</span>Species &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">one_hot</span>(record<span class="op">$</span>Species, 3L)</span>
<span id="cb15-4"><a href="#cb15-4"></a>    record</span>
<span id="cb15-5"><a href="#cb15-5"></a>  }) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb15-6"><a href="#cb15-6"></a><span class="st">  </span><span class="kw">datset_prefetch</span>(<span class="dv">1</span>)</span></code></pre></div>
<p>If you are batching your data for training, you can optimize performance using the <code>dataset_map_and_batch()</code> function (which fuses together the map and batch operations). For example:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1"></a>dataset &lt;-<span class="st"> </span>dataset <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb16-2"><a href="#cb16-2"></a><span class="st">  </span><span class="kw">dataset_map_and_batch</span>(<span class="dt">batch_size =</span> <span class="dv">128</span>, <span class="cf">function</span>(record) {</span>
<span id="cb16-3"><a href="#cb16-3"></a>    record<span class="op">$</span>Species &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">one_hot</span>(record<span class="op">$</span>Species, 3L)</span>
<span id="cb16-4"><a href="#cb16-4"></a>    record</span>
<span id="cb16-5"><a href="#cb16-5"></a>  }) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb16-6"><a href="#cb16-6"></a><span class="st">  </span><span class="kw">datset_prefetch</span>(<span class="dv">1</span>)</span></code></pre></div>
</div>
</div>
<div id="filtering" class="section level3">
<h3>Filtering</h3>
<p>You can filter the elements of a dataset using the <code>dataset_filter()</code> function, which takes a <code>predicate</code> function that returns a boolean tensor for records that should be included. For example:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1"></a>dataset &lt;-<span class="st"> </span><span class="kw">csv_dataset</span>(<span class="st">&quot;mtcars.csv&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb17-2"><a href="#cb17-2"></a><span class="st">  </span><span class="kw">dataset_filter</span>(<span class="cf">function</span>(record) {</span>
<span id="cb17-3"><a href="#cb17-3"></a>    record<span class="op">$</span>mpg <span class="op">&gt;=</span><span class="st"> </span><span class="dv">20</span></span>
<span id="cb17-4"><a href="#cb17-4"></a>})</span>
<span id="cb17-5"><a href="#cb17-5"></a></span>
<span id="cb17-6"><a href="#cb17-6"></a>dataset &lt;-<span class="st"> </span><span class="kw">csv_dataset</span>(<span class="st">&quot;mtcars.csv&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb17-7"><a href="#cb17-7"></a><span class="st">  </span><span class="kw">dataset_filter</span>(<span class="cf">function</span>(record) {</span>
<span id="cb17-8"><a href="#cb17-8"></a>    record<span class="op">$</span>mpg <span class="op">&gt;=</span><span class="st"> </span><span class="dv">20</span> <span class="op">&amp;</span><span class="st"> </span>record<span class="op">$</span>cyl <span class="op">&gt;=</span><span class="st"> </span>6L</span>
<span id="cb17-9"><a href="#cb17-9"></a>  })</span></code></pre></div>
<p>Note that the functions used inside the predicate must be tensor operations (e.g. <code>tf$not_equal</code>, <code>tf$less</code>, etc.). R generic methods for relational operators (e.g. &lt;, &gt;, &lt;=, etc.) and logical operators (e.g. !, &amp;, |, etc.) are provided so you can use shorthand syntax for most common comparisons (as illustrated above).</p>
</div>
<div id="features-and-response" class="section level3">
<h3>Features and Response</h3>
<p>A common transformation is taking a column oriented dataset (e.g. one created by <code>csv_dataset()</code> or <code>tfrecord_dataset()</code>) and transforming it into a two-element list with features (“x”) and response (“y”). You can use the <code>dataset_prepare()</code> function to do this type of transformation. For example:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1"></a>mtcars_dataset &lt;-<span class="st"> </span><span class="kw">text_line_dataset</span>(<span class="st">&quot;mtcars.csv&quot;</span>, <span class="dt">record_spec =</span> mtcars_spec) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb18-2"><a href="#cb18-2"></a><span class="st">  </span><span class="kw">dataset_prepare</span>(<span class="dt">x =</span> <span class="kw">c</span>(mpg, disp), <span class="dt">y =</span> cyl)</span>
<span id="cb18-3"><a href="#cb18-3"></a></span>
<span id="cb18-4"><a href="#cb18-4"></a>iris_dataset &lt;-<span class="st"> </span><span class="kw">text_line_dataset</span>(<span class="st">&quot;iris.csv&quot;</span>, <span class="dt">record_spec =</span> iris_spec) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb18-5"><a href="#cb18-5"></a><span class="st">  </span><span class="kw">dataset_prepare</span>(<span class="dt">x =</span> <span class="op">-</span>Species, <span class="dt">y =</span> Species)</span></code></pre></div>
<p>The <code>dataset_prepare()</code> function also accepts standard R formula syntax for defining features and response:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1"></a>mtcars_dataset &lt;-<span class="st"> </span><span class="kw">text_line_dataset</span>(<span class="st">&quot;mtcars.csv&quot;</span>, <span class="dt">record_spec =</span> mtcars_spec) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb19-2"><a href="#cb19-2"></a><span class="st">  </span><span class="kw">dataset_prepare</span>(cyl <span class="op">~</span><span class="st"> </span>mpg <span class="op">+</span><span class="st"> </span>disp)</span></code></pre></div>
<p>If you are batching your data for training you add a <code>batch_size</code> parameter to fuse together the <code>dataset_prepare()</code> and <code>dataset_batch()</code> steps (which generally results in faster training). For example:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1"></a>mtcars_dataset &lt;-<span class="st"> </span><span class="kw">text_line_dataset</span>(<span class="st">&quot;mtcars.csv&quot;</span>, <span class="dt">record_spec =</span> mtcars_spec) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb20-2"><a href="#cb20-2"></a><span class="st">  </span><span class="kw">dataset_prepare</span>(cyl <span class="op">~</span><span class="st"> </span>mpg <span class="op">+</span><span class="st"> </span>disp, <span class="dt">batch_size =</span> <span class="dv">16</span>)</span></code></pre></div>
</div>
<div id="shuffling-and-batching" class="section level3">
<h3>Shuffling and Batching</h3>
<p>There are several functions which control how batches are drawn from the dataset. For example, the following specifies that data will be drawn in batches of 128 from a shuffled window of 1000 records, and that the dataset will be repeated for 10 epochs:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1"></a>dataset &lt;-<span class="st"> </span>dataset <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb21-2"><a href="#cb21-2"></a><span class="st">  </span><span class="kw">dataset_shuffle</span>(<span class="dv">1000</span>) <span class="op">%&gt;%</span></span>
<span id="cb21-3"><a href="#cb21-3"></a><span class="st">  </span><span class="kw">dataset_repeat</span>(<span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb21-4"><a href="#cb21-4"></a><span class="st">  </span><span class="kw">dataset_batch</span>(<span class="dv">128</span>) <span class="op">%&gt;%</span><span class="st"> </span></span></code></pre></div>
<p>Note that you can optimize performance by fusing the shuffle and repeat operations into a single step using the <code>dataset_shuffle_and_repeat()</code> function. For example:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1"></a>dataset &lt;-<span class="st"> </span>dataset <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb22-2"><a href="#cb22-2"></a><span class="st">  </span><span class="kw">dataset_shuffle_and_repeat</span>(<span class="dt">buffer_size =</span> <span class="dv">1000</span>, <span class="dt">count =</span> <span class="dv">10</span>) <span class="op">%&gt;%</span></span>
<span id="cb22-3"><a href="#cb22-3"></a><span class="st">  </span><span class="kw">dataset_batch</span>(<span class="dv">128</span>)</span></code></pre></div>
</div>
<div id="prefetching" class="section level3">
<h3>Prefetching</h3>
<p>Earlier we alluded to the <code>dataset_prefetch()</code> function, which enables you to ensure that a given number of records (or batches of records) are prefetched in parallel so they are ready to go when the next batch is processed. For example:</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1"></a>dataset &lt;-<span class="st"> </span>dataset <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb23-2"><a href="#cb23-2"></a><span class="st">  </span><span class="kw">dataset_map_and_batch</span>(<span class="dt">batch_size =</span> <span class="dv">128</span>, <span class="cf">function</span>(record) {</span>
<span id="cb23-3"><a href="#cb23-3"></a>    record<span class="op">$</span>Species &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">one_hot</span>(record<span class="op">$</span>Species, 3L)</span>
<span id="cb23-4"><a href="#cb23-4"></a>    record</span>
<span id="cb23-5"><a href="#cb23-5"></a>  }) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb23-6"><a href="#cb23-6"></a><span class="st">  </span><span class="kw">dataset_prefetch</span>(<span class="dv">1</span>)</span></code></pre></div>
<p>If you are using a GPU for training, you can also use the <code>dataset_prefetch_to_device()</code> function to specify that the parallel prefetch operation stage the data directly into GPU memory. For example:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1"></a>dataset &lt;-<span class="st"> </span>dataset <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb24-2"><a href="#cb24-2"></a><span class="st">  </span><span class="kw">dataset_map_and_batch</span>(<span class="dt">batch_size =</span> <span class="dv">128</span>, <span class="cf">function</span>(record) {</span>
<span id="cb24-3"><a href="#cb24-3"></a>    record<span class="op">$</span>Species &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">one_hot</span>(record<span class="op">$</span>Species, 3L)</span>
<span id="cb24-4"><a href="#cb24-4"></a>    record</span>
<span id="cb24-5"><a href="#cb24-5"></a>  }) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb24-6"><a href="#cb24-6"></a><span class="st">  </span><span class="kw">dataset_prefetch_to_device</span>(<span class="st">&quot;/gpu:0&quot;</span>)</span></code></pre></div>
<p>In this case the buffer size for prefetches is determined automatically (you can manually speicfy it using the <code>buffer_size</code> parameter).</p>
</div>
<div id="complete-example" class="section level3">
<h3>Complete Example</h3>
<p>Here’s a complete example of using the various dataset transformation functions together. We’ll read the <code>mtcars</code> dataset from a CSV, filter it on some threshold values, map it into <code>x</code> and <code>y</code> components for modeling, and specify desired shuffling and batch iteration behavior:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1"></a>dataset &lt;-<span class="st"> </span><span class="kw">text_line_dataset</span>(<span class="st">&quot;mtcars.csv&quot;</span>, <span class="dt">record_spec =</span> mtcars_spec) <span class="op">%&gt;%</span></span>
<span id="cb25-2"><a href="#cb25-2"></a><span class="st">  </span><span class="kw">dataset_filter</span>(<span class="cf">function</span>(record) {</span>
<span id="cb25-3"><a href="#cb25-3"></a>    record<span class="op">$</span>mpg <span class="op">&gt;=</span><span class="st"> </span><span class="dv">20</span> <span class="op">&amp;</span><span class="st"> </span>record<span class="op">$</span>cyl <span class="op">&gt;=</span><span class="st"> </span>6L</span>
<span id="cb25-4"><a href="#cb25-4"></a>  }) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb25-5"><a href="#cb25-5"></a><span class="st">  </span><span class="kw">dataset_shuffle_and_repeat</span>(<span class="dt">buffer_size =</span> <span class="dv">1000</span>, <span class="dt">count =</span> <span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb25-6"><a href="#cb25-6"></a><span class="st">  </span><span class="kw">dataset_prepare</span>(cyl <span class="op">~</span><span class="st"> </span>mpg <span class="op">+</span><span class="st"> </span>disp, <span class="dt">batch_size =</span> <span class="dv">128</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb25-7"><a href="#cb25-7"></a><span class="st">  </span><span class="kw">dataset_prefetch</span>(<span class="dv">1</span>)</span></code></pre></div>
</div>
</div>
<div id="reading-datasets" class="section level2">
<h2>Reading Datasets</h2>
<p>The method for reading data from a TensorFlow Dataset varies depending upon which API you are using to build your models. If you are using the <a href="https://tensorflow.rstudio.com/keras">keras</a>, then TensorFlow Datasets can be used much like in-memory R matrices and arrays. If you are using the lower-level <a href="https://tensorflow.rstudio.com/tensorflow/">tensorflow core</a> API then you’ll use explicit dataset iteration functions.</p>
<p>The sections below provide additional details and examples for each of the supported APIs.</p>
<div id="keras-package" class="section level3">
<h3>keras package</h3>
<p><strong>IMPORTANT NOTE</strong>: Using TensorFlow Datasets with Keras requires that you are running the very latest versions of Keras (v2.2) and TensorFlow (v1.9). You can ensure that you have the latest versions of the core Keras and TensorFlow libraries with:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1"></a><span class="kw">library</span>(keras)</span>
<span id="cb26-2"><a href="#cb26-2"></a><span class="kw">install_keras</span>()</span></code></pre></div>
<p>Keras models are often trained by passing in-memory arrays directly to the <code>fit</code> function. For example:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1"></a>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(</span>
<span id="cb27-2"><a href="#cb27-2"></a>  x_train, y_train, </span>
<span id="cb27-3"><a href="#cb27-3"></a>  <span class="dt">epochs =</span> <span class="dv">30</span>, </span>
<span id="cb27-4"><a href="#cb27-4"></a>  <span class="dt">batch_size =</span> <span class="dv">128</span></span>
<span id="cb27-5"><a href="#cb27-5"></a>)</span></code></pre></div>
<p>However, this requires loading data into an R data frame or matrix before calling fit. You can use the <code>train_on_batch()</code> function to stream data one batch at a time, however the reading and processing of the input data is still being done serially and outside of native code.</p>
<p>Alternatively, Keras enables you to pass a dataset directly as the <code>x</code> argument to <code>fit()</code> and <code>evaluate()</code>. Here’s a complete example that uses datasets to read from TFRecord files containing MNIST digits:</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1"></a><span class="kw">library</span>(keras)</span>
<span id="cb28-2"><a href="#cb28-2"></a><span class="kw">library</span>(tfdatasets)</span>
<span id="cb28-3"><a href="#cb28-3"></a></span>
<span id="cb28-4"><a href="#cb28-4"></a>batch_size =<span class="st"> </span><span class="dv">128</span></span>
<span id="cb28-5"><a href="#cb28-5"></a>steps_per_epoch =<span class="st"> </span><span class="dv">500</span></span>
<span id="cb28-6"><a href="#cb28-6"></a></span>
<span id="cb28-7"><a href="#cb28-7"></a><span class="co"># function to read and preprocess mnist dataset</span></span>
<span id="cb28-8"><a href="#cb28-8"></a>mnist_dataset &lt;-<span class="st"> </span><span class="cf">function</span>(filename) {</span>
<span id="cb28-9"><a href="#cb28-9"></a>  dataset &lt;-<span class="st"> </span><span class="kw">tfrecord_dataset</span>(filename) <span class="op">%&gt;%</span></span>
<span id="cb28-10"><a href="#cb28-10"></a><span class="st">    </span><span class="kw">dataset_map</span>(<span class="cf">function</span>(example_proto) {</span>
<span id="cb28-11"><a href="#cb28-11"></a></span>
<span id="cb28-12"><a href="#cb28-12"></a>      <span class="co"># parse record</span></span>
<span id="cb28-13"><a href="#cb28-13"></a>      features &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">parse_single_example</span>(</span>
<span id="cb28-14"><a href="#cb28-14"></a>        example_proto,</span>
<span id="cb28-15"><a href="#cb28-15"></a>        <span class="dt">features =</span> <span class="kw">list</span>(</span>
<span id="cb28-16"><a href="#cb28-16"></a>          <span class="dt">image_raw =</span> tf<span class="op">$</span><span class="kw">FixedLenFeature</span>(<span class="kw">shape</span>(), tf<span class="op">$</span>string),</span>
<span id="cb28-17"><a href="#cb28-17"></a>          <span class="dt">label =</span> tf<span class="op">$</span><span class="kw">FixedLenFeature</span>(<span class="kw">shape</span>(), tf<span class="op">$</span>int64)</span>
<span id="cb28-18"><a href="#cb28-18"></a>        )</span>
<span id="cb28-19"><a href="#cb28-19"></a>      )</span>
<span id="cb28-20"><a href="#cb28-20"></a></span>
<span id="cb28-21"><a href="#cb28-21"></a>      <span class="co"># preprocess image</span></span>
<span id="cb28-22"><a href="#cb28-22"></a>      image &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">decode_raw</span>(features<span class="op">$</span>image_raw, tf<span class="op">$</span>uint8)</span>
<span id="cb28-23"><a href="#cb28-23"></a>      image &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">cast</span>(image, tf<span class="op">$</span>float32) <span class="op">/</span><span class="st"> </span><span class="dv">255</span></span>
<span id="cb28-24"><a href="#cb28-24"></a></span>
<span id="cb28-25"><a href="#cb28-25"></a>      <span class="co"># convert label to one-hot</span></span>
<span id="cb28-26"><a href="#cb28-26"></a>      label &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">one_hot</span>(tf<span class="op">$</span><span class="kw">cast</span>(features<span class="op">$</span>label, tf<span class="op">$</span>int32), 10L)</span>
<span id="cb28-27"><a href="#cb28-27"></a></span>
<span id="cb28-28"><a href="#cb28-28"></a>      <span class="co"># return</span></span>
<span id="cb28-29"><a href="#cb28-29"></a>      <span class="kw">list</span>(image, label)</span>
<span id="cb28-30"><a href="#cb28-30"></a>    }) <span class="op">%&gt;%</span></span>
<span id="cb28-31"><a href="#cb28-31"></a><span class="st">    </span><span class="kw">dataset_repeat</span>() <span class="op">%&gt;%</span></span>
<span id="cb28-32"><a href="#cb28-32"></a><span class="st">    </span><span class="kw">dataset_shuffle</span>(<span class="dv">1000</span>) <span class="op">%&gt;%</span></span>
<span id="cb28-33"><a href="#cb28-33"></a><span class="st">    </span><span class="kw">dataset_batch</span>(batch_size, <span class="dt">drop_remainder =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></span>
<span id="cb28-34"><a href="#cb28-34"></a><span class="st">    </span><span class="kw">dataset_prefetch</span>(<span class="dv">1</span>)</span>
<span id="cb28-35"><a href="#cb28-35"></a>}</span>
<span id="cb28-36"><a href="#cb28-36"></a></span>
<span id="cb28-37"><a href="#cb28-37"></a>model &lt;-<span class="st"> </span><span class="kw">keras_model_sequential</span>() <span class="op">%&gt;%</span></span>
<span id="cb28-38"><a href="#cb28-38"></a><span class="st">  </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">256</span>, <span class="dt">activation =</span> <span class="st">&#39;relu&#39;</span>, <span class="dt">input_shape =</span> <span class="kw">c</span>(<span class="dv">784</span>)) <span class="op">%&gt;%</span></span>
<span id="cb28-39"><a href="#cb28-39"></a><span class="st">  </span><span class="kw">layer_dropout</span>(<span class="dt">rate =</span> <span class="fl">0.4</span>) <span class="op">%&gt;%</span></span>
<span id="cb28-40"><a href="#cb28-40"></a><span class="st">  </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">128</span>, <span class="dt">activation =</span> <span class="st">&#39;relu&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb28-41"><a href="#cb28-41"></a><span class="st">  </span><span class="kw">layer_dropout</span>(<span class="dt">rate =</span> <span class="fl">0.3</span>) <span class="op">%&gt;%</span></span>
<span id="cb28-42"><a href="#cb28-42"></a><span class="st">  </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">10</span>, <span class="dt">activation =</span> <span class="st">&#39;softmax&#39;</span>)</span>
<span id="cb28-43"><a href="#cb28-43"></a></span>
<span id="cb28-44"><a href="#cb28-44"></a>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">compile</span>(</span>
<span id="cb28-45"><a href="#cb28-45"></a>  <span class="dt">loss =</span> <span class="st">&#39;categorical_crossentropy&#39;</span>,</span>
<span id="cb28-46"><a href="#cb28-46"></a>  <span class="dt">optimizer =</span> <span class="kw">optimizer_rmsprop</span>(),</span>
<span id="cb28-47"><a href="#cb28-47"></a>  <span class="dt">metrics =</span> <span class="kw">c</span>(<span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb28-48"><a href="#cb28-48"></a>)</span>
<span id="cb28-49"><a href="#cb28-49"></a></span>
<span id="cb28-50"><a href="#cb28-50"></a>history &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(</span>
<span id="cb28-51"><a href="#cb28-51"></a>  <span class="kw">mnist_dataset</span>(<span class="st">&quot;mnist/train.tfrecords&quot;</span>),</span>
<span id="cb28-52"><a href="#cb28-52"></a>  <span class="dt">steps_per_epoch =</span> steps_per_epoch,</span>
<span id="cb28-53"><a href="#cb28-53"></a>  <span class="dt">epochs =</span> <span class="dv">20</span>,</span>
<span id="cb28-54"><a href="#cb28-54"></a>  <span class="dt">validation_data =</span> <span class="kw">mnist_dataset</span>(<span class="st">&quot;mnist/validation.tfrecords&quot;</span>),</span>
<span id="cb28-55"><a href="#cb28-55"></a>  <span class="dt">validation_steps =</span> steps_per_epoch</span>
<span id="cb28-56"><a href="#cb28-56"></a>)</span>
<span id="cb28-57"><a href="#cb28-57"></a></span>
<span id="cb28-58"><a href="#cb28-58"></a>score &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">evaluate</span>(</span>
<span id="cb28-59"><a href="#cb28-59"></a>  <span class="kw">mnist_dataset</span>(<span class="st">&quot;mnist/test.tfrecords&quot;</span>),</span>
<span id="cb28-60"><a href="#cb28-60"></a>  <span class="dt">steps =</span> steps_per_epoch</span>
<span id="cb28-61"><a href="#cb28-61"></a>)</span>
<span id="cb28-62"><a href="#cb28-62"></a></span>
<span id="cb28-63"><a href="#cb28-63"></a><span class="kw">print</span>(score)</span></code></pre></div>
<p>Note that all data preprocessing (e.g. one-hot encoding of the response variable) is done within the <code>dataset_map()</code> operation.</p>
<p>Also note that we pass <code>drop_remainder = TRUE</code> to the <code>dataset_batch()</code> function (this is to make sure that all batches are of equal size, a requirement for Keras tensor inputs).</p>
</div>
<div id="tensorflow-package" class="section level3">
<h3>tensorflow package</h3>
<p>You read batches of data from a dataset by using tensors that yield the next batch. You can obtain this tensor from a dataset via the <code>make_iterator_one_shot()</code> and <code>iterator_get_next()</code> functions. For example:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1"></a>dataset &lt;-<span class="st"> </span><span class="kw">text_line_dataset</span>(<span class="st">&quot;mtcars.csv&quot;</span>, <span class="dt">record_spec =</span> mtcars_spec) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb29-2"><a href="#cb29-2"></a><span class="st">  </span><span class="kw">dataset_prepare</span>(cyl <span class="op">~</span><span class="st"> </span>mpg <span class="op">+</span><span class="st"> </span>disp) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb29-3"><a href="#cb29-3"></a><span class="st">  </span><span class="kw">dataset_shuffle</span>(<span class="dv">20</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb29-4"><a href="#cb29-4"></a><span class="st">  </span><span class="kw">dataset_batch</span>(<span class="dv">5</span>)</span>
<span id="cb29-5"><a href="#cb29-5"></a></span>
<span id="cb29-6"><a href="#cb29-6"></a>iter &lt;-<span class="st"> </span><span class="kw">make_iterator_one_shot</span>(dataset)</span>
<span id="cb29-7"><a href="#cb29-7"></a>next_batch &lt;-<span class="st"> </span><span class="kw">iterator_get_next</span>(iter)</span>
<span id="cb29-8"><a href="#cb29-8"></a>next_batch</span></code></pre></div>
<pre><code>$x
Tensor(&quot;IteratorGetNext_13:0&quot;, shape=(?, 2), dtype=float32)

$y
Tensor(&quot;IteratorGetNext_13:1&quot;, shape=(?,), dtype=int32)</code></pre>
<p>As you can see <code>next_batch</code> isn’t the data itself but rather a tensor that will yield the next batch of data when it is evaluated:</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1"></a>sess &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">Session</span>()</span>
<span id="cb31-2"><a href="#cb31-2"></a>sess<span class="op">$</span><span class="kw">run</span>(next_batch)</span></code></pre></div>
<pre><code>$x
     [,1] [,2]
[1,] 21.0  160
[2,] 21.0  160
[3,] 22.8  108
[4,] 21.4  258
[5,] 18.7  360

$y
[1] 6 6 4 6 8</code></pre>
<p>If you are iterating over a dataset using these functions, you will need to determine at what point to stop iteration. One approach to this is to use the <code>dataset_repeat()</code> function to create an dataset that yields values infinitely. For example:</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1"></a><span class="kw">library</span>(tfdatasets)</span>
<span id="cb33-2"><a href="#cb33-2"></a></span>
<span id="cb33-3"><a href="#cb33-3"></a>sess &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">Session</span>()</span>
<span id="cb33-4"><a href="#cb33-4"></a></span>
<span id="cb33-5"><a href="#cb33-5"></a>mtcars_spec &lt;-<span class="st"> </span><span class="kw">csv_record_spec</span>(<span class="st">&quot;mtcars.csv&quot;</span>)</span>
<span id="cb33-6"><a href="#cb33-6"></a>dataset &lt;-<span class="st"> </span><span class="kw">text_line_dataset</span>(<span class="st">&quot;mtcars.csv&quot;</span>, <span class="dt">record_spec =</span> mtcars_spec) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb33-7"><a href="#cb33-7"></a><span class="st">  </span><span class="kw">dataset_shuffle</span>(<span class="dv">5000</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb33-8"><a href="#cb33-8"></a><span class="st">  </span><span class="kw">dataset_repeat</span>() <span class="co"># repeat infinitely</span></span>
<span id="cb33-9"><a href="#cb33-9"></a>  <span class="kw">dataset_prepare</span>(<span class="dt">x =</span> <span class="kw">c</span>(mpg, disp), <span class="dt">y =</span> cyl) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb33-10"><a href="#cb33-10"></a><span class="st">  </span><span class="kw">dataset_batch</span>(<span class="dv">128</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb33-11"><a href="#cb33-11"></a></span>
<span id="cb33-12"><a href="#cb33-12"></a>iter &lt;-<span class="st"> </span><span class="kw">make_iterator_one_shot</span>(dataset)</span>
<span id="cb33-13"><a href="#cb33-13"></a>next_batch &lt;-<span class="st"> </span><span class="kw">iterator_get_next</span>(iter)</span>
<span id="cb33-14"><a href="#cb33-14"></a></span>
<span id="cb33-15"><a href="#cb33-15"></a>steps &lt;-<span class="st"> </span><span class="dv">200</span></span>
<span id="cb33-16"><a href="#cb33-16"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>steps) {</span>
<span id="cb33-17"><a href="#cb33-17"></a>  </span>
<span id="cb33-18"><a href="#cb33-18"></a>  <span class="co"># use next_batch for training, etc. </span></span>
<span id="cb33-19"><a href="#cb33-19"></a>  </span>
<span id="cb33-20"><a href="#cb33-20"></a>  <span class="co"># (note that you need to actually use the next_batch e.g. by passing it to a</span></span>
<span id="cb33-21"><a href="#cb33-21"></a>  <span class="co"># function that consumes a tensor or by running it explicitly) in order to </span></span>
<span id="cb33-22"><a href="#cb33-22"></a>  <span class="co"># advance to the next batch)</span></span>
<span id="cb33-23"><a href="#cb33-23"></a>}</span></code></pre></div>
<p>In this case the <code>steps</code> variable is used to determine when to stop drawing new batches of training data (we could have equally included code to detect a learning plateau or any other custom method of determining when to stop training).</p>
<p>Another approach is to detect when all batches have been yielded from the dataset. When a dataset iterator reaches the end, an out of range runtime error will occur. You can catch and ignore the error when it occurs by using <code>out_of_range_handler</code> as the <code>error</code> argument to <code>tryCatch()</code>. For example:</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1"></a><span class="kw">library</span>(tfdatasets)</span>
<span id="cb34-2"><a href="#cb34-2"></a></span>
<span id="cb34-3"><a href="#cb34-3"></a>sess &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">Session</span>()</span>
<span id="cb34-4"><a href="#cb34-4"></a></span>
<span id="cb34-5"><a href="#cb34-5"></a>mtcars_spec &lt;-<span class="st"> </span><span class="kw">csv_record_spec</span>(<span class="st">&quot;mtcars.csv&quot;</span>)</span>
<span id="cb34-6"><a href="#cb34-6"></a>dataset &lt;-<span class="st"> </span><span class="kw">text_line_dataset</span>(<span class="st">&quot;mtcars.csv&quot;</span>, <span class="dt">record_spec =</span> mtcars_spec) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb34-7"><a href="#cb34-7"></a><span class="st">  </span><span class="kw">dataset_prepare</span>(<span class="dt">x =</span> <span class="kw">c</span>(mpg, disp), <span class="dt">y =</span> cyl) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb34-8"><a href="#cb34-8"></a><span class="st">  </span><span class="kw">dataset_batch</span>(<span class="dv">128</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb34-9"><a href="#cb34-9"></a><span class="st">  </span><span class="kw">dataset_repeat</span>(<span class="dv">10</span>)</span>
<span id="cb34-10"><a href="#cb34-10"></a>  </span>
<span id="cb34-11"><a href="#cb34-11"></a>iter &lt;-<span class="st"> </span><span class="kw">make_iterator_one_shot</span>(dataset)</span>
<span id="cb34-12"><a href="#cb34-12"></a>next_batch &lt;-<span class="st"> </span><span class="kw">iterator_get_next</span>(iter)</span>
<span id="cb34-13"><a href="#cb34-13"></a></span>
<span id="cb34-14"><a href="#cb34-14"></a><span class="kw">tryCatch</span>({</span>
<span id="cb34-15"><a href="#cb34-15"></a>  <span class="cf">while</span>(<span class="ot">TRUE</span>) {</span>
<span id="cb34-16"><a href="#cb34-16"></a>    batch &lt;-<span class="st"> </span>sess<span class="op">$</span><span class="kw">run</span>(next_batch)</span>
<span id="cb34-17"><a href="#cb34-17"></a>    <span class="kw">str</span>(batch)</span>
<span id="cb34-18"><a href="#cb34-18"></a>  }</span>
<span id="cb34-19"><a href="#cb34-19"></a>}, <span class="dt">error =</span> out_of_range_handler)</span></code></pre></div>
<p>You can write this iteration more elegantly using the <code>until_out_of_range()</code> function, which automatically handles the error and provides the <code>while(TRUE)</code> around an expression:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1"></a><span class="kw">until_out_of_range</span>({</span>
<span id="cb35-2"><a href="#cb35-2"></a>  batch &lt;-<span class="st"> </span>sess<span class="op">$</span><span class="kw">run</span>(next_batch)</span>
<span id="cb35-3"><a href="#cb35-3"></a>  <span class="kw">str</span>(batch)</span>
<span id="cb35-4"><a href="#cb35-4"></a>})</span></code></pre></div>
<p>When running under eager execution, you organize the code a bit differently (since you don’t need to explicitly <code>run()</code> tensors):</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1"></a>iter &lt;-<span class="st"> </span><span class="kw">make_iterator_one_shot</span>(dataset)</span>
<span id="cb36-2"><a href="#cb36-2"></a></span>
<span id="cb36-3"><a href="#cb36-3"></a><span class="kw">until_out_of_range</span>({</span>
<span id="cb36-4"><a href="#cb36-4"></a>  batch &lt;-<span class="st"> </span><span class="kw">iterator_get_next</span>(iter)</span>
<span id="cb36-5"><a href="#cb36-5"></a>  <span class="kw">str</span>(batch)</span>
<span id="cb36-6"><a href="#cb36-6"></a>})</span></code></pre></div>
</div>
</div>
<div id="reading-multiple-files" class="section level2">
<h2>Reading Multiple Files</h2>
<p>If you have multiple input files you can process them in parallel both across machines (sharding) and/or on multiple threads per-machine (parallel reads with interleaving). The <code>read_files()</code> function provides a high-level interface to parallel file reading.</p>
<p>The <code>read_files()</code> function takes a set of files and a read function along with various options to orchestrate parallel reading. For example, the following function reads all CSV files in a directory using the <code>text_line_dataset()</code> function:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1"></a>dataset &lt;-<span class="st"> </span><span class="kw">read_files</span>(<span class="st">&quot;data/*.csv&quot;</span>, text_line_dataset, <span class="dt">record_spec =</span> mtcars_spec,</span>
<span id="cb37-2"><a href="#cb37-2"></a>                      <span class="dt">parallel_files =</span> <span class="dv">4</span>, <span class="dt">parallel_interleave =</span> <span class="dv">16</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb37-3"><a href="#cb37-3"></a><span class="st">  </span><span class="kw">dataset_prefetch</span>(<span class="dv">5000</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb37-4"><a href="#cb37-4"></a><span class="st">  </span><span class="kw">dataset_shuffle_and_repeat</span>(<span class="dt">buffer_size =</span> <span class="dv">1000</span>, <span class="dt">count =</span> <span class="dv">3</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb37-5"><a href="#cb37-5"></a><span class="st">  </span><span class="kw">dataset_batch</span>(<span class="dv">128</span>)</span></code></pre></div>
<p>The <code>parallel_files</code> argument requests that 4 files be processed in parallel and the <code>parallel_interleave</code> argument requests that blocks of 16 consecutive records from each file be interleaved in the resulting dataset.</p>
<p>Note that because we are processing files in parallel we <em>do not</em> pass the <code>parallel_records</code> argument to <code>text_line_dataset()</code>, since we are already parallelizing at the file level.</p>
<div id="multiple-machines" class="section level3">
<h3>Multiple Machines</h3>
<p>If you are training on multiple machines and the training supervisor passes a shard index to your training script, you can also parallelizing reading by sharding the file list. For example:</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1"></a><span class="co"># command line flags for training script (shard info is passed by training </span></span>
<span id="cb38-2"><a href="#cb38-2"></a><span class="co"># supervisor that executes the script)</span></span>
<span id="cb38-3"><a href="#cb38-3"></a>FLAGS &lt;-<span class="st"> </span><span class="kw">flags</span>(</span>
<span id="cb38-4"><a href="#cb38-4"></a>  <span class="kw">flag_integer</span>(<span class="st">&quot;num_shards&quot;</span>, <span class="dv">1</span>),</span>
<span id="cb38-5"><a href="#cb38-5"></a>  <span class="kw">flag_integer</span>(<span class="st">&quot;shard_index&quot;</span>, <span class="dv">1</span>)</span>
<span id="cb38-6"><a href="#cb38-6"></a>)</span>
<span id="cb38-7"><a href="#cb38-7"></a></span>
<span id="cb38-8"><a href="#cb38-8"></a><span class="co"># forward shard info to read_files</span></span>
<span id="cb38-9"><a href="#cb38-9"></a>dataset &lt;-<span class="st"> </span><span class="kw">read_files</span>(<span class="st">&quot;data/*.csv&quot;</span>, text_line_dataset, <span class="dt">record_spec =</span> mtcars_spec,</span>
<span id="cb38-10"><a href="#cb38-10"></a>                      <span class="dt">parallel_files =</span> <span class="dv">4</span>, <span class="dt">parallel_interleave =</span> <span class="dv">16</span>,</span>
<span id="cb38-11"><a href="#cb38-11"></a>                      <span class="dt">num_shards =</span> FLAGS<span class="op">$</span>num_shards, <span class="dt">shard_index =</span> FLAGS<span class="op">$</span>shard_index) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb38-12"><a href="#cb38-12"></a><span class="st">  </span><span class="kw">dataset_shuffle_and_repeat</span>(<span class="dt">buffer_size =</span> <span class="dv">1000</span>, <span class="dt">count =</span> <span class="dv">3</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb38-13"><a href="#cb38-13"></a><span class="st">  </span><span class="kw">dataset_batch</span>(<span class="dv">128</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb38-14"><a href="#cb38-14"></a><span class="st">  </span><span class="kw">dataset_prefetch</span>(<span class="dv">1</span>)</span></code></pre></div>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
